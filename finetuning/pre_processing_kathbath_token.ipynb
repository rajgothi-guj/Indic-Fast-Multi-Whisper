{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-processing part\n",
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/hdd/Gothi_raj/Whisper/dataset/kathbath/kb_data_clean_wav/gujarati/train/bucket.csv\"\n",
    "# dev_path = \"/hdd/Gothi_raj/Whisper/dataset/kathbath/kb_data_clean_wav/gujarati/valid/bucket.csv\"\n",
    "text_save_path = '../dataset/kathbath/kb_data_clean_wav/gujarati/all_text.txt'\n",
    "tokenizer_save_path = \"../tokenizer/gujarati_tokenizer_1000\"\n",
    "\n",
    "\n",
    "audio = []\n",
    "transcript= []\n",
    "\n",
    "flg=0\n",
    "with open(train_path,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # print(line)\n",
    "        if flg==0:        \n",
    "            flg=1\n",
    "            continue\n",
    "        line = line.split(',')\n",
    "        audio.append(line[0])\n",
    "        transcript.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'path':audio,'transcription':transcript})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev_path = \"/raid/speech/rajgothi/whisper/kathbath/kb_data_clean_wav/gujarati/valid/bucket.csv\"\n",
    "\n",
    "# audio = []\n",
    "# transcript= []\n",
    "\n",
    "# flg=0\n",
    "# with open(dev_path,'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         # print(line)\n",
    "#         if flg==0:        \n",
    "#             flg=1\n",
    "#             continue\n",
    "#         line = line.split(',')\n",
    "#         audio.append(line[0])\n",
    "#         transcript.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_df = pd.DataFrame({'path':audio,'transcription':transcript})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd/Gothi_raj/envs/wt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'transcription'],\n",
       "        num_rows: 71063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict,Dataset\n",
    "\n",
    "train_data_hf = Dataset.from_pandas(train_df)\n",
    "# dev_data_hf = Dataset.from_pandas(dev_df)\n",
    "\n",
    "# dataset = DatasetDict({'train':train_data_hf,'validation':dev_data_hf})\n",
    "dataset = DatasetDict({'train':train_data_hf})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcription = dataset['train']['transcription']\n",
    "# validation_transcription = dataset['validation']['transcription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcription = set(train_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_save_path,'w') as f:\n",
    "    for sentence in train_transcription:\n",
    "        f.write(sentence+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='', vocab_size=1000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new tokenizer\n",
    "from tokenizers import (decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer)\n",
    "from transformers import GPT2Tokenizer, GPT2TokenizerFast, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "\n",
    "new_tokenizer = Tokenizer(models.BPE())\n",
    "new_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "trainer = trainers.BpeTrainer(vocab_size=1000, special_tokens=[\"<|endoftext|>\"])\n",
    "train_file = text_save_path\n",
    "new_tokenizer.train([train_file], trainer=trainer)\n",
    "new_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "new_tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "new_tokenizer = GPT2TokenizerFast(tokenizer_object=new_tokenizer)\n",
    "new_tokenizer.save_pretrained(\"new_tokenizer_gpt2\")\n",
    "new_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "1000 1000\n",
      "51199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "[156, 103, 228, 220, 156, 103, 99, 156, 103, 113, 156, 103, 122, 156, 103, 241, 220, 156, 103, 116, 156, 103, 108, 156, 104, 235, 156, 103, 250, 156, 103, 108, 156, 104, 222, 220, 156, 103, 99, 156, 103, 108, 156, 103, 106, 156, 103, 123, 156, 103, 107, 156, 103, 122, 156, 103, 101, 220, 156, 103, 250, 156, 104, 235, 156, 103, 252]\n",
      "16\n",
      "[124, 555, 62, 123, 215, 65, 195, 67, 280, 75, 74, 62, 66, 98, 65, 410]\n",
      "16\n",
      "[50429, 50477, 50762, 50265, 51013, 51086, 50687, 50873, 50724, 50580, 50706, 50762, 50452, 51000, 51086, 50673]\n"
     ]
    }
   ],
   "source": [
    "# gpt2 tokenizer\n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(len(gpt2_tokenizer.get_vocab()))\n",
    "gpt2_tokenizer\n",
    "\n",
    "# merge the vocabulary for the extended tokenizer\n",
    "vocab_tokens = list(new_tokenizer.get_vocab())\n",
    "decoded_tokens = [new_tokenizer.decoder.decode([token]) for token in vocab_tokens]\n",
    "print(len(vocab_tokens), len(decoded_tokens))\n",
    "gpt2_tokenizer.add_tokens(decoded_tokens)\n",
    "\n",
    "gpt2_tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(len(gpt2_tokenizer.get_vocab()))\n",
    "gpt2_tokenizer\n",
    "\n",
    "# validate the changes\n",
    "text = \"આ દવાઓ સર્જરી દરમિયાન જ્ઞ\"\n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "new_tokenizer = GPT2TokenizerFast.from_pretrained(\"new_tokenizer_gpt2\")\n",
    "extended_tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "print(len(gpt2_tokenizer.encode(text)))\n",
    "print(gpt2_tokenizer.encode(text))\n",
    "print(len(new_tokenizer.encode(text)))\n",
    "print(new_tokenizer.encode(text))\n",
    "print(len(extended_tokenizer.encode(text)))\n",
    "print(extended_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' પરન',\n",
       " 'નજ',\n",
       " 'યદ',\n",
       " ' મહત',\n",
       " 'દલ',\n",
       " 'ઉત',\n",
       " ' લગભગ',\n",
       " ' પડશ',\n",
       " 'ઓ',\n",
       " ' નહ',\n",
       " 'મન',\n",
       " ' રહ',\n",
       " 'સક',\n",
       " 'લવ',\n",
       " ' આપવ',\n",
       " ' અડ',\n",
       " 'ચલ',\n",
       " ' પચ',\n",
       " 'દન',\n",
       " 'ખવ',\n",
       " 'હન',\n",
       " 'ઓએ',\n",
       " 'ડક',\n",
       " 'લક',\n",
       " 'એન',\n",
       " 'ળવવ',\n",
       " ' બત',\n",
       " 'હમ',\n",
       " 'વધ',\n",
       " '�',\n",
       " 'ૈ',\n",
       " ' ઓર',\n",
       " ' તળ',\n",
       " ' એજન',\n",
       " ' આઈ',\n",
       " 'ઓથ',\n",
       " 'ટન',\n",
       " ' વસ',\n",
       " ' તપ',\n",
       " ' તસવ',\n",
       " 'મસ',\n",
       " ' ભર',\n",
       " 'પથ',\n",
       " 'ડ',\n",
       " ' ઉઠ',\n",
       " 'યમ',\n",
       " ' રચન',\n",
       " 'ૂં',\n",
       " ' આઇ',\n",
       " ' દરવ',\n",
       " 'એટલ',\n",
       " 'બળ',\n",
       " 'કથ',\n",
       " ' ઑ',\n",
       " 'કવણ',\n",
       " ' ભય',\n",
       " ' એર',\n",
       " ' ઉજવણ',\n",
       " 'તદ',\n",
       " 'પવ',\n",
       " 'ગસ',\n",
       " 'ધર',\n",
       " 'ગડ',\n",
       " 'ષત',\n",
       " ' વર',\n",
       " 'લર',\n",
       " 'ટરમ',\n",
       " ' અમદ',\n",
       " ' જળ',\n",
       " 'મણ',\n",
       " ' જગ',\n",
       " ' ઓફ',\n",
       " ' ઘર',\n",
       " 'પજ',\n",
       " ' હ',\n",
       " ' અખ',\n",
       " ' સન',\n",
       " ' શર',\n",
       " 'ઇન',\n",
       " 'અમ',\n",
       " '�',\n",
       " ' બસ',\n",
       " ' રશ',\n",
       " 'ભળ',\n",
       " 'ઃ',\n",
       " 'શનલ',\n",
       " 'ળન',\n",
       " 'પ',\n",
       " ' હક',\n",
       " 'બઈ',\n",
       " ' ભક',\n",
       " 'સઠ',\n",
       " 'દરન',\n",
       " 'યરન',\n",
       " ' અસરક',\n",
       " ' ઓ',\n",
       " 'ૂ',\n",
       " ' ખસ',\n",
       " ' આશ',\n",
       " ' ઇ',\n",
       " 'જપન',\n",
       " ' પડ',\n",
       " 'ળમ',\n",
       " ' મથક',\n",
       " 'નથ',\n",
       " ' નબળ',\n",
       " 'રણ',\n",
       " ' કપ',\n",
       " '�',\n",
       " ' શ',\n",
       " ' એન',\n",
       " 'ટલમ',\n",
       " 'લગ',\n",
       " ' ઇમ',\n",
       " 'તરપ',\n",
       " 'ઈવ',\n",
       " '�',\n",
       " ' થય',\n",
       " ' આપણ',\n",
       " ' કરત',\n",
       " ' દ',\n",
       " ' બર',\n",
       " 'શનમ',\n",
       " 'નપ',\n",
       " '�',\n",
       " 'ઈ',\n",
       " 'જવ',\n",
       " ' થશ',\n",
       " 'ુઃ',\n",
       " 'ગત',\n",
       " 'રલ',\n",
       " '�',\n",
       " ' ઉપલબ',\n",
       " ' પદ',\n",
       " ' મળત',\n",
       " ' ઐ',\n",
       " 'વવ',\n",
       " '�',\n",
       " 'યરલ',\n",
       " 'કર',\n",
       " 'ખત',\n",
       " 'વસમ',\n",
       " '�',\n",
       " ' પક',\n",
       " ' એ',\n",
       " ' હજ',\n",
       " ' એમન',\n",
       " ' એપ',\n",
       " ' ઈ',\n",
       " 'ૃ',\n",
       " ' પશ',\n",
       " ' એમ',\n",
       " ' લક',\n",
       " 'ટક',\n",
       " 'ગન',\n",
       " 'વનમ',\n",
       " 'ીં',\n",
       " 'રથમ',\n",
       " 'ષન',\n",
       " 'એમ',\n",
       " ' થઇ',\n",
       " 'ઓમ',\n",
       " ' નવ',\n",
       " ' શન',\n",
       " 'ચમ',\n",
       " ' શખ',\n",
       " ' બ',\n",
       " 'વસ',\n",
       " ' સલ',\n",
       " ' આફ',\n",
       " 'ખલ',\n",
       " 'ડમ',\n",
       " 'થવ',\n",
       " 'થક',\n",
       " 'લમ',\n",
       " 'નદ',\n",
       " ' પલ',\n",
       " ' સદ',\n",
       " ' પથ',\n",
       " 'આ',\n",
       " 'શ',\n",
       " 'રમણ',\n",
       " ' બનવ',\n",
       " ' ટક',\n",
       " 'પરવ',\n",
       " 'ધ',\n",
       " 'જસ',\n",
       " 'ડસ',\n",
       " ' ઉમ',\n",
       " ' વગર',\n",
       " 'વસન',\n",
       " 'સર',\n",
       " 'કશ',\n",
       " ' ઓસ',\n",
       " ' રક',\n",
       " ' ફક',\n",
       " 'ું',\n",
       " 'ટલ',\n",
       " ' ગઇ',\n",
       " ' રકમ',\n",
       " ' થ',\n",
       " ' એકત',\n",
       " 'ન',\n",
       " 'તરન',\n",
       " 'છળ',\n",
       " ' નર',\n",
       " 'ઇએ',\n",
       " ' મજ',\n",
       " ' ગમ',\n",
       " ' ખ',\n",
       " ' ઉપય',\n",
       " 'ખર',\n",
       " 'અગ',\n",
       " 'મધ',\n",
       " 'ડલ',\n",
       " 'ઝ',\n",
       " 'જય',\n",
       " 'અપ',\n",
       " ' ભગવ',\n",
       " '�',\n",
       " ' ઉતર',\n",
       " 'યટ',\n",
       " ' અજ',\n",
       " 'જલ',\n",
       " 'સમ',\n",
       " 'રગ',\n",
       " 'ટથ',\n",
       " 'તચ',\n",
       " ' દવ',\n",
       " ' આત',\n",
       " ' ઉચ',\n",
       " 'ણ',\n",
       " 'આઈ',\n",
       " 'રભ',\n",
       " ' પહ',\n",
       " ' જમ',\n",
       " 'તક',\n",
       " '�',\n",
       " ' હર',\n",
       " ' ઉગ',\n",
       " 'રચ',\n",
       " 'મચ',\n",
       " 'ધમ',\n",
       " ' જનત',\n",
       " ' અદ',\n",
       " 'ઈપણ',\n",
       " 'મલ',\n",
       " 'કત',\n",
       " 'ડળ',\n",
       " ' સ',\n",
       " 'નગર',\n",
       " '�',\n",
       " 'ઈક',\n",
       " 'વચ',\n",
       " ' ભરત',\n",
       " '�',\n",
       " ' આવવ',\n",
       " 'કલ',\n",
       " 'રત',\n",
       " ' મળ',\n",
       " 'મત',\n",
       " 'ષ',\n",
       " ' વ',\n",
       " 'રખ',\n",
       " ' ઇજ',\n",
       " 'નર',\n",
       " 'દરત',\n",
       " ' તહ',\n",
       " 'કપ',\n",
       " ' મસ',\n",
       " 'આમ',\n",
       " ' ઘટ',\n",
       " 'જમ',\n",
       " 'ઝન',\n",
       " 'ગરમ',\n",
       " 'ફર',\n",
       " ' આભ',\n",
       " 'યવહ',\n",
       " 'વક',\n",
       " ' ભવ',\n",
       " ' ગત',\n",
       " ' દક',\n",
       " 'આગ',\n",
       " ' ભજવ',\n",
       " ' મગ',\n",
       " 'બઈમ',\n",
       " ' ભ',\n",
       " ' મજબ',\n",
       " 'વજ',\n",
       " ' રદ',\n",
       " 'એસ',\n",
       " 'બદ',\n",
       " 'વત',\n",
       " 'લત',\n",
       " 'ઈલ',\n",
       " 'લય',\n",
       " ' દબ',\n",
       " 'દક',\n",
       " ' એટ',\n",
       " ' એલર',\n",
       " ' મધ',\n",
       " 'કળ',\n",
       " 'ળખ',\n",
       " 'નત',\n",
       " ' સમયથ',\n",
       " ' ઝડપથ',\n",
       " 'ઉ',\n",
       " ' નક',\n",
       " 'ષમ',\n",
       " ' સરખ',\n",
       " 'ક',\n",
       " '<|endoftext|>',\n",
       " ' આવન',\n",
       " 'હદ',\n",
       " 'ઇવ',\n",
       " 'ઠ',\n",
       " 'આપણ',\n",
       " ' આવક',\n",
       " 'ચવ',\n",
       " 'વણ',\n",
       " ' હત',\n",
       " 'કડ',\n",
       " ' મહ',\n",
       " '�',\n",
       " ' પસ',\n",
       " 'પકડ',\n",
       " 'વળ',\n",
       " 'જક',\n",
       " 'ઉન',\n",
       " 'ટસ',\n",
       " 'પત',\n",
       " 'ઘન',\n",
       " 'લ',\n",
       " 'ટણ',\n",
       " ' હશ',\n",
       " ' બધ',\n",
       " 'િ',\n",
       " ' ર',\n",
       " 'ગઢ',\n",
       " ' અટક',\n",
       " 'ઉટ',\n",
       " '�',\n",
       " ' ઋ',\n",
       " 'આઇ',\n",
       " 'દ',\n",
       " ' ઈચ',\n",
       " 'ં',\n",
       " ' ચલ',\n",
       " ' પણ',\n",
       " 'રફત',\n",
       " ' પરત',\n",
       " 'હ',\n",
       " 'ઘ',\n",
       " 'તરર',\n",
       " '�',\n",
       " 'તવ',\n",
       " 'અત',\n",
       " ' જન',\n",
       " ' છત',\n",
       " ' આકર',\n",
       " 'ત',\n",
       " ' આવત',\n",
       " ' અત',\n",
       " '�',\n",
       " 'રય',\n",
       " ' એલ',\n",
       " 'રષ',\n",
       " ' અવ',\n",
       " 'ફળ',\n",
       " '�',\n",
       " ' અભ',\n",
       " '�',\n",
       " 'ર',\n",
       " ' જત',\n",
       " 'સથ',\n",
       " ' ચ',\n",
       " 'યલ',\n",
       " ' ચહ',\n",
       " ' આવ',\n",
       " ' ખબર',\n",
       " ' નગર',\n",
       " 'ગવ',\n",
       " ' કર',\n",
       " 'ગ',\n",
       " ' પટ',\n",
       " 'ણવ',\n",
       " ' અમ',\n",
       " 'કમ',\n",
       " ' અક',\n",
       " 'વપ',\n",
       " 'વ',\n",
       " ' ત',\n",
       " ' ઓફર',\n",
       " 'ળકન',\n",
       " 'તથ',\n",
       " ' રસપ',\n",
       " 'રહ',\n",
       " ' રસ',\n",
       " 'ઘટન',\n",
       " 'વરણ',\n",
       " ' મતદ',\n",
       " 'મમ',\n",
       " ' આલ',\n",
       " 'શન',\n",
       " ' એટલ',\n",
       " 'યમન',\n",
       " ' આપશ',\n",
       " 'તળ',\n",
       " 'પણ',\n",
       " ' તત',\n",
       " ' રમ',\n",
       " 'ઈન',\n",
       " ' આધ',\n",
       " '�',\n",
       " 'ભવ',\n",
       " 'ણય',\n",
       " 'રધ',\n",
       " ' ધરપકડ',\n",
       " 'ફ',\n",
       " 'વન',\n",
       " ' કબ',\n",
       " 'વનન',\n",
       " ' આપન',\n",
       " 'ણસ',\n",
       " ' મન',\n",
       " 'ે',\n",
       " ' ગણ',\n",
       " 'સવ',\n",
       " ' કમ',\n",
       " 'ણક',\n",
       " ' સજ',\n",
       " ' આખ',\n",
       " '�',\n",
       " 'ખન',\n",
       " ' ઓપર',\n",
       " 'નક',\n",
       " 'ઞ',\n",
       " ' ઠ',\n",
       " 'મદ',\n",
       " ' સતત',\n",
       " ' ઇચ',\n",
       " ' સફળ',\n",
       " 'ષણ',\n",
       " 'ળક',\n",
       " ' બહ',\n",
       " '�',\n",
       " ' શહ',\n",
       " ' સત',\n",
       " 'ઈએ',\n",
       " 'મર',\n",
       " ' વગ',\n",
       " 'જર',\n",
       " ' જવ',\n",
       " ' લગ',\n",
       " ' સમસ',\n",
       " 'હવ',\n",
       " 'જન',\n",
       " ' જર',\n",
       " '�',\n",
       " 'ાં',\n",
       " ' લખ',\n",
       " 'છપરછ',\n",
       " ' ભરવ',\n",
       " 'ઈટ',\n",
       " ' તણ',\n",
       " 'છલ',\n",
       " 'ઇટ',\n",
       " 'અર',\n",
       " 'પન',\n",
       " ' અપ',\n",
       " 'લસ',\n",
       " 'ય',\n",
       " 'ટરન',\n",
       " '\\n',\n",
       " 'કલવ',\n",
       " 'કસભ',\n",
       " 'ગશ',\n",
       " ' અથવ',\n",
       " 'આજ',\n",
       " ' સવ',\n",
       " ' અચ',\n",
       " 'શમ',\n",
       " 'ળત',\n",
       " 'યણ',\n",
       " ' ખર',\n",
       " 'અહ',\n",
       " 'હક',\n",
       " 'બજ',\n",
       " ' વડ',\n",
       " ' દરમ',\n",
       " ' લઇ',\n",
       " ' ઈન',\n",
       " 'રજન',\n",
       " 'રક',\n",
       " ' ડ',\n",
       " 'ષણન',\n",
       " ' ઓનલ',\n",
       " 'રણમ',\n",
       " ' વધત',\n",
       " 'થર',\n",
       " ' કરવ',\n",
       " ' તમ',\n",
       " 'મહ',\n",
       " 'ઠળ',\n",
       " 'ગઠ',\n",
       " ' છવ',\n",
       " 'રવ',\n",
       " ' કવ',\n",
       " ' જઈ',\n",
       " '�',\n",
       " 'રથ',\n",
       " 'રબ',\n",
       " ' જય',\n",
       " 'બલ',\n",
       " 'બધ',\n",
       " 'નન',\n",
       " ' વળ',\n",
       " 'ઓન',\n",
       " 'ણવત',\n",
       " 'હર',\n",
       " ' પગલ',\n",
       " 'થ',\n",
       " 'સ',\n",
       " 'આન',\n",
       " 'બહ',\n",
       " ' ભલ',\n",
       " 'ભર',\n",
       " 'ઘણ',\n",
       " 'ા',\n",
       " 'શભ',\n",
       " 'યર',\n",
       " 'લયન',\n",
       " 'ધત',\n",
       " ' ઉપસ',\n",
       " 'પહ',\n",
       " 'ફન',\n",
       " 'ણન',\n",
       " 'પષ',\n",
       " 'છત',\n",
       " 'ોં',\n",
       " 'યત',\n",
       " '�',\n",
       " 'તરમ',\n",
       " ' રન',\n",
       " 'યવ',\n",
       " 'થન',\n",
       " '�',\n",
       " ' પ',\n",
       " '�',\n",
       " 'આથ',\n",
       " ' વક',\n",
       " '�',\n",
       " ' ઇન',\n",
       " ' આગ',\n",
       " '�',\n",
       " ' ઘટન',\n",
       " ' કન',\n",
       " ' ઊ',\n",
       " ' ઘરમ',\n",
       " 'શનન',\n",
       " ' વખત',\n",
       " ' મદ',\n",
       " 'ઝર',\n",
       " ' આઠ',\n",
       " 'વભ',\n",
       " 'ણમ',\n",
       " 'દય',\n",
       " ' વરસ',\n",
       " 'બર',\n",
       " ' ધ',\n",
       " '�',\n",
       " 'રફ',\n",
       " ' તર',\n",
       " 'વટ',\n",
       " 'કરન',\n",
       " ' ગ',\n",
       " ' સફળત',\n",
       " ' આય',\n",
       " 'દમ',\n",
       " ' રવ',\n",
       " 'ટર',\n",
       " ' આમ',\n",
       " ' એક',\n",
       " ' પડક',\n",
       " ' ઉભ',\n",
       " ' અફ',\n",
       " ' ઇલ',\n",
       " ' ઉપ',\n",
       " ' બળ',\n",
       " 'રદર',\n",
       " 'ઉપર',\n",
       " 'દવ',\n",
       " 'ળ',\n",
       " ' થવ',\n",
       " ' ',\n",
       " 'ટ',\n",
       " ' ફ',\n",
       " '�',\n",
       " 'નગ',\n",
       " 'ઇમ',\n",
       " '�',\n",
       " '�',\n",
       " 'ખમ',\n",
       " ' ઉક',\n",
       " 'ગથ',\n",
       " 'તપ',\n",
       " ' વખ',\n",
       " ' ઘણ',\n",
       " ' ગર',\n",
       " 'ચક',\n",
       " ' આહ',\n",
       " ' સમયગ',\n",
       " 'ધવ',\n",
       " 'ઇઝ',\n",
       " 'કટર',\n",
       " ' તથ',\n",
       " 'ગણ',\n",
       " ' એશ',\n",
       " ' ઉન',\n",
       " ' કરશ',\n",
       " 'વશ',\n",
       " ' ઓક',\n",
       " '�',\n",
       " ' નદ',\n",
       " ' પગ',\n",
       " ' પછ',\n",
       " '�',\n",
       " 'ડવ',\n",
       " 'જ',\n",
       " 'પમ',\n",
       " ' ફર',\n",
       " ' જણ',\n",
       " '�',\n",
       " ' થઈ',\n",
       " ' અ',\n",
       " ' ફળ',\n",
       " 'બમ',\n",
       " ' પરમ',\n",
       " 'ઘર',\n",
       " 'હત',\n",
       " ' આપત',\n",
       " ' આ',\n",
       " 'યવસ',\n",
       " 'છ',\n",
       " 'ઉલ',\n",
       " 'ડન',\n",
       " ' એકમ',\n",
       " ' ઘ',\n",
       " 'ભગ',\n",
       " ' લડ',\n",
       " ' જલ',\n",
       " 'ી',\n",
       " 'સત',\n",
       " 'ુ',\n",
       " ' પત',\n",
       " '�',\n",
       " '�',\n",
       " ' એવ',\n",
       " 'લન',\n",
       " ' કડક',\n",
       " 'સપ',\n",
       " ' મદદ',\n",
       " 'ઈડ',\n",
       " ' શબ',\n",
       " 'ઠકમ',\n",
       " ' તરત',\n",
       " ' ઔ',\n",
       " '�',\n",
       " 'દગ',\n",
       " 'એવ',\n",
       " 'અમદ',\n",
       " ' હળવ',\n",
       " ' અઢ',\n",
       " 'યન',\n",
       " ' ઉજવ',\n",
       " 'નવ',\n",
       " 'ષક',\n",
       " ' કચ',\n",
       " 'દર',\n",
       " 'હરણ',\n",
       " ' પર',\n",
       " 'ગરન',\n",
       " ' મલ',\n",
       " ' સમર',\n",
       " ' ઓછ',\n",
       " 'દરમ',\n",
       " 'ડત',\n",
       " ' મચ',\n",
       " ' અલ',\n",
       " '�',\n",
       " 'રપત',\n",
       " 'ટબ',\n",
       " 'વમ',\n",
       " 'કભ',\n",
       " 'મ',\n",
       " ' અમન',\n",
       " 'સદ',\n",
       " ' ફટક',\n",
       " 'ેં',\n",
       " ' કશ',\n",
       " ' અશ',\n",
       " ' ઇત',\n",
       " ' હન',\n",
       " ' કટ',\n",
       " ' કસ',\n",
       " ' વજન',\n",
       " ' નથ',\n",
       " ' એકબ',\n",
       " ' બચ',\n",
       " ' લ',\n",
       " ' ટ',\n",
       " 'શર',\n",
       " ' હટ',\n",
       " ' ઓગણ',\n",
       " 'અન',\n",
       " ' સમજ',\n",
       " 'મજ',\n",
       " 'હલ',\n",
       " ' ન',\n",
       " ' ઉલ',\n",
       " ' દલ',\n",
       " ' આજ',\n",
       " 'ઝમ',\n",
       " 'થમ',\n",
       " ' છ',\n",
       " ' રજ',\n",
       " 'પડ',\n",
       " ' રમત',\n",
       " ' ઝડપ',\n",
       " 'એ',\n",
       " '�',\n",
       " ' એસ',\n",
       " ' ચક',\n",
       " 'ૌ',\n",
       " ' થત',\n",
       " 'મહત',\n",
       " 'તર',\n",
       " '�',\n",
       " 'નમ',\n",
       " ' વહ',\n",
       " ' કરન',\n",
       " ' તરફથ',\n",
       " ' નજ',\n",
       " ' ઉડ',\n",
       " 'ષમત',\n",
       " 'ચત',\n",
       " 'ઇ',\n",
       " ' ઓળખ',\n",
       " 'ઠક',\n",
       " ' સભ',\n",
       " ' પકડ',\n",
       " ' ઈજ',\n",
       " 'યસ',\n",
       " 'ો',\n",
       " 'કવ',\n",
       " 'પસ',\n",
       " ' શકત',\n",
       " 'ટમ',\n",
       " 'ળવ',\n",
       " 'રમ',\n",
       " ' ગરમ',\n",
       " ' પરવ',\n",
       " 'ચ',\n",
       " ' તક',\n",
       " 'રમમ',\n",
       " 'સરન',\n",
       " 'જનક',\n",
       " ' અર',\n",
       " '�',\n",
       " 'યરસ',\n",
       " ' ગઈ',\n",
       " 'ડર',\n",
       " 'તરણ',\n",
       " '�',\n",
       " 'ગલ',\n",
       " ' સફ',\n",
       " 'ઢવ',\n",
       " 'સન',\n",
       " 'પલ',\n",
       " ' ઓળખવ',\n",
       " ' ઓળ',\n",
       " 'લબ',\n",
       " ' સરળત',\n",
       " 'પર',\n",
       " ' કહ',\n",
       " ' બગ',\n",
       " ' જ',\n",
       " ' જશ',\n",
       " ' અગ',\n",
       " '�',\n",
       " 'ખ',\n",
       " 'આત',\n",
       " ' છબ',\n",
       " 'શહ',\n",
       " 'ઠવ',\n",
       " ' દસ',\n",
       " ' સપ',\n",
       " 'ધપ',\n",
       " 'તમ',\n",
       " 'થળ',\n",
       " ' સર',\n",
       " 'ટફ',\n",
       " ' વધ',\n",
       " 'જપ',\n",
       " ' લઇન',\n",
       " ' દર',\n",
       " 'અ',\n",
       " ' અન',\n",
       " ' સચ',\n",
       " 'ણત',\n",
       " 'વલ',\n",
       " 'ધક',\n",
       " 'તન',\n",
       " ' ઘરન',\n",
       " ' હવ',\n",
       " ' ગય',\n",
       " ' લઈન',\n",
       " 'મવ',\n",
       " 'ઇડ',\n",
       " ' ફરજ',\n",
       " 'રતમ',\n",
       " 'યરસન',\n",
       " 'કન',\n",
       " ' સસ',\n",
       " 'ઇસ',\n",
       " 'વર',\n",
       " 'િં',\n",
       " ' આન',\n",
       " 'ડપ',\n",
       " ' ચર',\n",
       " ' સરક',\n",
       " ' અડધ',\n",
       " 'બરન',\n",
       " 'રમન',\n",
       " ' અકસ',\n",
       " '�',\n",
       " ' શક',\n",
       " ' સક',\n",
       " ' સમગ',\n",
       " 'ચર',\n",
       " 'સલ',\n",
       " 'પછ',\n",
       " 'એક',\n",
       " ' વધવ',\n",
       " 'વરન',\n",
       " ' સમયમ',\n",
       " ' સરળ',\n",
       " '�',\n",
       " 'કરણ',\n",
       " 'પદ',\n",
       " ' બદલ',\n",
       " ' નજર',\n",
       " ' આવશ',\n",
       " ' તમન',\n",
       " ' તકલ',\n",
       " ' ચડ',\n",
       " 'વડ',\n",
       " 'ઇલ',\n",
       " '�',\n",
       " ' તરફ',\n",
       " ' બન',\n",
       " ' અવસ',\n",
       " ' તબ',\n",
       " ' એકવ',\n",
       " ' અધ',\n",
       " 'પચ',\n",
       " 'ભ',\n",
       " '�',\n",
       " 'જબ',\n",
       " '�',\n",
       " ' કય',\n",
       " 'રસ',\n",
       " ' ધરવ',\n",
       " 'ઉદ',\n",
       " ' ઉદ',\n",
       " ' આક',\n",
       " ' મ',\n",
       " ' શકશ',\n",
       " '્',\n",
       " 'તમજ',\n",
       " 'જગ',\n",
       " 'એલ',\n",
       " ' સમ',\n",
       " 'ળજ',\n",
       " 'ચન',\n",
       " ' ઝ',\n",
       " ' ગણતર',\n",
       " '�',\n",
       " 'રન',\n",
       " ' આર',\n",
       " 'તમન',\n",
       " ' ક',\n",
       " ' આદ',\n",
       " 'મક',\n",
       " 'સબ',\n",
       " ' લઈ',\n",
       " 'સણ',\n",
       " ' બજ',\n",
       " 'ગર',\n",
       " 'આવ',\n",
       " '�',\n",
       " 'બ',\n",
       " 'ગળવ',\n",
       " ' હસ',\n",
       " ' ધર',\n",
       " ' ગણવ',\n",
       " ' ઉ',\n",
       " ' મળશ',\n",
       " 'જનન',\n",
       " 'છપર',\n",
       " 'લભ',\n",
       " ' આસપ',\n",
       " ' મનમ',\n",
       " 'નલ',\n",
       " ' પરથ',\n",
       " '�',\n",
       " 'કસ',\n",
       " ' વલ',\n",
       " 'ૅ',\n",
       " 'મથ',\n",
       " 'ગળ',\n",
       " ' અઠવ',\n",
       " ' કલ',\n",
       " 'ટવ',\n",
       " ' ઢ',\n",
       " ' કથ',\n",
       " ' �',\n",
       " '�',\n",
       " ' પડય',\n",
       " 'પક',\n",
       " ' અલગ',\n",
       " ' સહ',\n",
       " 'મગ',\n",
       " 'રજ',\n",
       " ' મર',\n",
       " ' આપ',\n",
       " ' વન',\n",
       " 'રતન',\n",
       " 'આર',\n",
       " ' આદર',\n",
       " ' પડત',\n",
       " ' એરપ',\n",
       " '�',\n",
       " ' અસ',\n",
       " ' અસર',\n",
       " 'યપ',\n",
       " ' મક',\n",
       " 'રયત',\n",
       " 'યક',\n",
       " '�',\n",
       " ' ય',\n",
       " 'યકર',\n",
       " ' ઉત',\n",
       " ' અહ',\n",
       " 'બન',\n",
       " ' આગળ',\n",
       " 'ષય',\n",
       " ' ઊભ',\n",
       " ' કદ',\n",
       " 'અલ',\n",
       " ' ઉપકરણ',\n",
       " ' એડ',\n",
       " 'પટ',\n",
       " ' અરજ',\n",
       " 'સરક',\n",
       " ' સમય',\n",
       " 'રડ',\n",
       " 'રપ',\n",
       " ' મદદથ',\n",
       " 'ઇક',\n",
       " '�',\n",
       " 'ડય',\n",
       " ' બનત',\n",
       " 'નસ',\n",
       " 'મડ',\n",
       " ' તબક',\n",
       " '�',\n",
       " ' વચ',\n",
       " 'નસભ',\n",
       " ' અમલ',\n",
       " ' મત',\n",
       " 'શક',\n",
       " ' ઉપર',\n",
       " ' આસ',\n",
       " 'કટ',\n",
       " 'ણપણ',\n",
       " 'રણન',\n",
       " 'રદ',\n",
       " ' મળવ',\n",
       " 'બત',\n",
       " 'સભ',\n",
       " ' વપર',\n",
       " 'ધન',\n",
       " 'રશ',\n",
       " ' કપડ',\n",
       " 'ઈમ',\n",
       " 'ઢ',\n",
       " 'ગમ',\n",
       " '�',\n",
       " 'બસ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barakhadi =  '''\n",
    "# ા\tિ\tી\tુ\tૂ\tે\tૈ\tો\tૌ\tં\tઃ\n",
    "# ક\tકા\tકિ\tકી\tકુ\tકૂ\tકે\tકૈ\tકો\tકૌ\tકં\tકઃ\n",
    "# ખ\tખા\tખિ\tખી\tખુ\tખૂ\tખે\tખૈ\tખો\tખૌ\tખં\tખઃ\n",
    "# ગ\tગા\tગિ\tગી\tગુ\tગૂ\tગે\tગૈ\tગો\tગૌ\tગં\tગઃ\n",
    "# ઘ\tઘા\tઘિ\tઘી\tઘુ\tઘૂ\tઘે\tઘૈ\tઘો\tઘૌ\tઘં\tઘઃ\n",
    "# ચ\tચા\tચિ\tચી\tચુ\tચૂ\tચે\tચૈ\tચો\tચૌ\tચં\tચઃ\n",
    "# છ\tછા\tછિ\tછી\tછુ\tછૂ\tછે\tછૈ\tછો\tછૌ\tછં\tછઃ\n",
    "# જ\tજા\tજિ\tજી\tજુ\tજૂ\tજે\tજૈ\tજો\tજૌ\tજં\tજઃ\n",
    "# ઝ\tઝા\tઝિ\tઝી\tઝુ\tઝૂ\tઝે\tઝૈ\tઝો\tઝૌ\tઝં\tઝઃ\n",
    "# ટ\tટા\tટિ\tટી\tટુ\tટૂ\tટે\tટૈ\tટો\tટૌ\tટં\tટઃ\n",
    "# ઠ\tઠા\tઠિ\tઠી\tઠુ\tઠૂ\tઠે\tઠૈ\tઠો\tઠૌ\tઠં\tઠઃ\n",
    "# ડ\tડા\tડિ\tડી\tડુ\tડૂ\tડે\tડૈ\tડો\tડૌ\tડં\tડઃ\n",
    "# ઢ\tઢા\tઢિ\tઢી\tઢુ\tઢૂ\tઢે\tઢૈ\tઢો\tઢૌ\tઢં\tઢઃ\n",
    "# ણ\tણા\tણિ\tણી\tણુ\tણૂ\tણે\tણૈ\tણો\tણૌ\tણં\tણઃ\n",
    "# ત\tતા\tતિ\tતી\tતુ\tતૂ\tતે\tતૈ\tતો\tતૌ\tતં\tતઃ\n",
    "# થ\tથા\tથિ\tથી\tથુ\tથૂ\tથે\tથૈ\tથો\tથૌ\tથં\tથઃ\n",
    "# દ\tદા\tદિ\tદી\tદુ\tદૂ\tદે\tદૈ\tદો\tદૌ\tદં\tદઃ\n",
    "# ધ\tધા\tધિ\tધી\tધુ\tધૂ\tધે\tધૈ\tધો\tધૌ\tધં\tધઃ\n",
    "# ન\tના\tનિ\tની\tનુ\tનૂ\tને\tનૈ\tનો\tનૌ\tનં\tનઃ\n",
    "# પ\tપા\tપિ\tપી\tપુ\tપૂ\tપે\tપૈ\tપો\tપૌ\tપં\tપઃ\n",
    "# ફ\tફા\tફિ\tફી\tફુ\tફૂ\tફે\tફૈ\tફો\tફૌ\tફં\tફઃ\n",
    "# બ\tબા\tબિ\tબી\tબુ\tબૂ\tબે\tબૈ\tબો\tબૌ\tબં\tબઃ\n",
    "# ભ\tભા\tભિ\tભી\tભુ\tભૂ\tભે\tભૈ\tભો\tભૌ\tભં\tભઃ\n",
    "# મ\tમા\tમિ\tમી\tમુ\tમૂ\tમે\tમૈ\tમો\tમૌ\tમં\tમઃ\n",
    "# ય\tયા\tયિ\tયી\tયુ\tયૂ\tયે\tયૈ\tયો\tયૌ\tયં\tયઃ\n",
    "# ર\tરા\tરિ\tરી\tરુ\tરૂ\tરે\tરૈ\tરો\tરૌ\tરં\tરઃ\n",
    "# લ\tલા\tલિ\tલી\tલુ\tલૂ\tલે\tલૈ\tલો\tલૌ\tલં\tલઃ\n",
    "# વ\tવા\tવિ\tવી\tવુ\tવૂ\tવે\tવૈ\tવો\tવૌ\tવં\tવઃ\n",
    "# શ\tશા\tશિ\tશી\tશુ\tશૂ\tશે\tશૈ\tશો\tશૌ\tશં\tશઃ\n",
    "# ષ\tષા\tષિ\tષી\tષુ\tષૂ\tષે\tષૈ\tષો\tષૌ\tષં\tષઃ\n",
    "# સ\tસા\tસિ\tસી\tસુ\tસૂ\tસે\tસૈ\tસો\tસૌ\tસં\tસઃ\n",
    "# હ\tહા\tહિ\tહી\tહુ\tહૂ\tહે\tહૈ\tહો\tહૌ\tહં\tહઃ\n",
    "# ળ\tળા\tળિ\tળી\tળુ\tળૂ\tળે\tળૈ\tળો\tળૌ\tળં\tળઃ\n",
    "# ક્ષ\tક્ષા\tક્ષિ\tક્ષી\tક્ષુ\tક્ષૂ\tક્ષે\tક્ષૈ\tક્ષો\tક્ષૌ\tક્ષં\tક્ષઃ\n",
    "# જ્ઞ\tજ્ઞા\tજ્ઞિ\tજ્ઞી\tજ્ઞુ\tજ્ઞૂ\tજ્ઞે\tજ્ઞૈ\tજ્ઞો\tજ્ઞૌ\tજ્ઞં\tજ્ઞઃ\n",
    "# અ\t\n",
    "# આ\t\n",
    "# ઇ\t\n",
    "# ઈ\t\n",
    "# ઉ\n",
    "# ઊ\n",
    "# ઋ\n",
    "# એ\n",
    "# ઐ\t\n",
    "# ઓ\t\n",
    "# ઔ\t\n",
    "# અં\t\n",
    "# અ:\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barakhadi = barakhadi.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barakhadi_new = [f' {i}' for i in barakhadi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_tokens = barakhadi + barakhadi_new + decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('extended_tokenizer_gpt2/tokenizer_config.json',\n",
       " 'extended_tokenizer_gpt2/special_tokens_map.json',\n",
       " 'extended_tokenizer_gpt2/vocab.json',\n",
       " 'extended_tokenizer_gpt2/merges.txt',\n",
       " 'extended_tokenizer_gpt2/added_tokens.json',\n",
       " 'extended_tokenizer_gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# print(len(gpt2_tokenizer.get_vocab()))\n",
    "# gpt2_tokenizer\n",
    "# gpt2_tokenizer.add_tokens(decoded_tokens)\n",
    "# gpt2_tokenizer.save_pretrained(\"extended_tokenizer_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "1000 432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('extended_tokenizer_gpt2/tokenizer_config.json',\n",
       " 'extended_tokenizer_gpt2/special_tokens_map.json',\n",
       " 'extended_tokenizer_gpt2/vocab.json',\n",
       " 'extended_tokenizer_gpt2/merges.txt',\n",
       " 'extended_tokenizer_gpt2/added_tokens.json',\n",
       " 'extended_tokenizer_gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# # print(len(gpt2_tokenizer.get_vocab()))\n",
    "# # gpt2_tokenizer\n",
    "\n",
    "# # merge the vocabulary for the extended tokenizer\n",
    "# print(len(vocab_tokens), len(barakhadi))\n",
    "# gpt2_tokenizer.add_tokens(barakhadi)\n",
    "# gpt2_tokenizer.save_pretrained(\"extended_tokenizer_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "[156, 103, 228, 220, 156, 103, 99, 156, 103, 113, 156, 103, 122, 156, 103, 241, 220, 156, 103, 116, 156, 103, 108, 156, 104, 235, 156, 103, 250, 156, 103, 108, 156, 104, 222, 220, 156, 103, 99, 156, 103, 108, 156, 103, 106, 156, 103, 123, 156, 103, 107, 156, 103, 122, 156, 103, 101, 220, 156, 103, 250, 156, 104, 235, 156, 103, 252]\n",
      "13\n",
      "[50677, 51521, 50257, 50685, 51399, 51800, 51781, 50259, 51670, 50258, 50545, 50472, 51096]\n"
     ]
    }
   ],
   "source": [
    "# # text = \"આ દવાઓ સર્જરી દરમિયાન\"\n",
    "\n",
    "# text = \"આ દવાઓ સર્જરી દરમિયાન જ્ઞ\"\n",
    "# gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# extended_tokenizer = GPT2TokenizerFast.from_pretrained(\"extended_tokenizer_gpt2\")\n",
    "\n",
    "# print(len(gpt2_tokenizer.encode(text)))\n",
    "# print(gpt2_tokenizer.encode(text))\n",
    "# print(len(extended_tokenizer.encode(text)))\n",
    "# print(extended_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' દવાઓ સર્જરી દરમિયાન જ્ઞ'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extended_tokenizer.decode([50677, 51521, 50257, 50685, 51399, 51800, 51781, 50259, 51670, 50258, 50545, 50472])\n",
    "\n",
    "# extended_tokenizer.decode([51521,50257,50685,51399,51800,51781,50259,51670,50258,50545,50472,51096])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "model_path = \"openai/whisper-medium\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path, language=\"Gujarati\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52807\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../tokenizer/gujarati_tokenizer_1000/tokenizer_config.json',\n",
       " '../tokenizer/gujarati_tokenizer_1000/special_tokens_map.json',\n",
       " '../tokenizer/gujarati_tokenizer_1000/vocab.json',\n",
       " '../tokenizer/gujarati_tokenizer_1000/merges.txt',\n",
       " '../tokenizer/gujarati_tokenizer_1000/normalizer.json',\n",
       " '../tokenizer/gujarati_tokenizer_1000/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence length 51\n",
      "Input:                 વ્યવસાયોના બહુવિધ રેખાઓ ધરાવતા વ્યવસાયો માટે આવકને ફાળવવામાં આવવી જોઈએ\n",
      "Decoded w/ special:    <|startoftranscript|><|gu|><|transcribe|><|notimestamps|>વ્યવસાયોના બહુવિધ રેખાઓ ધરાવતા વ્યવસાયો માટે આવકને ફાળવવામાં આવવી જોઈએ<|endoftext|>\n",
      "Decoded w/out special: વ્યવસાયોના બહુવિધ રેખાઓ ધરાવતા વ્યવસાયો માટે આવકને ફાળવવામાં આવવી જોઈએ\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = dataset[\"train\"][0][\"transcription\"]\n",
    "\n",
    "# prompt = \"IndoAryan\"\n",
    "# prompt_ids = tokenizer.get_prompt_ids(prompt)\n",
    "\n",
    "labels = tokenizer(input_str).input_ids\n",
    "\n",
    "print('tokenized sentence length',len(labels))\n",
    "# tokenizer = WhisperTokenizer.from_pretrained(model_path, language=\"Bengali\", task=\"transcribe\")\n",
    "# tokenizer.add_tokens(decoded_tokens)\n",
    "# labels = [51, 71, 72, 82, 53552, 82, 53869, 340, 2455, 83, 50257]\n",
    "\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/hdd/Gothi_raj/Whisper/dataset/kathbath/kb_data_clean_wav/gujarati/train/audio/107/844424931458674-107-f.wav',\n",
       " 'transcription': 'વ્યવસાયોના બહુવિધ રેખાઓ ધરાવતા વ્યવસાયો માટે આવકને ફાળવવામાં આવવી જોઈએ'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = dataset['train']['path']\n",
    "# new_path = []\n",
    "# for path in train_path:\n",
    "#     path = path.split('/')\n",
    "#     path = '/'.join(path[:-1]) + '/train/' + path[-1]\n",
    "#     new_path.append(path)\n",
    "\n",
    "# dataset['train'] = dataset['train'].remove_columns(['path'])\n",
    "# dataset['train'] = dataset['train'].add_column(\"path\", new_path)\n",
    "\n",
    "# dev_path = dataset['validation']['path']\n",
    "# new_path = []\n",
    "# for path in dev_path:\n",
    "#     path = path.split('/')\n",
    "#     path = '/'.join(path[:-1]) + '/dev/' + path[-1]\n",
    "#     new_path.append(path)\n",
    "\n",
    "# dataset['validation'] = dataset['validation'].remove_columns(['path'])\n",
    "# dataset['validation'] = dataset['validation'].add_column(\"path\", new_path)\n",
    "\n",
    "\n",
    "# test_path = dataset['test']['path']\n",
    "# new_path = []\n",
    "# for path in test_path:\n",
    "#     path = path.split('/')\n",
    "#     path = '/'.join(path[:-1]) + '/test/' + path[-1]\n",
    "#     new_path.append(path)\n",
    "\n",
    "# dataset['test'] = dataset['test'].remove_columns(['path'])\n",
    "# dataset['test'] = dataset['test'].add_column(\"path\", new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# processor = WhisperProcessor.from_pretrained(model_path, language=\"Hindi\", task=\"transcribe\")\n",
    "\n",
    "processor = WhisperProcessor(tokenizer=tokenizer,feature_extractor=feature_extractor)\n",
    "processor.save_pretrained(\"Trained_model/kathbath_token_processor_gu_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'transcription'],\n",
       "        num_rows: 71063\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['path', 'transcription'],\n",
       "        num_rows: 3139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"path\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260970545274af6863d7e45d3682d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/71063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e0d450d5ed458aa08e5ae751f580e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/3139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 71063\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 3139\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62c835fa85c4f0b90fca0177e4ba5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/137 shards):   0%|          | 0/71063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec2aeac23c1435f8c30122bed08ff11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/7 shards):   0%|          | 0/3139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('Dataset/gujarati_kathbath_tokenized_1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 2120\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 239\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 418\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
